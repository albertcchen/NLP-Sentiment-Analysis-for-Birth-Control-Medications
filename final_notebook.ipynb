{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP For Drugs.com Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/albertcc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/albertcc/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "sns.color_palette(\"Blues\", as_cmap=True)\n",
    "\n",
    "### Standard Packages\n",
    "import numpy as np\n",
    "import warnings\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "### NLTK\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "import contractions\n",
    "\n",
    "### Scikit-Learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report, \\\n",
    "                            accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "### ImbLearn\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing in two .tsv files as test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/drugsComTest_raw.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-10d2a6949c71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load in the test and train datasets provided in the data file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/drugsComTest_raw.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/drugsComTrain_raw.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/drugsComTest_raw.tsv'"
     ]
    }
   ],
   "source": [
    "# Load in the test and train datasets provided in the data file\n",
    "data_test = pd.read_csv('dsc/data/drugsComTest_raw.tsv', sep='\\t')\n",
    "data_train = pd.read_csv('data/drugsComTrain_raw.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Test and Train dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data provided is already split into test and train tsv files. I would like to combine these to not only have more data to work with, but any cleaning could be applied to the merged dataset before splitting into a training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([data_test, data_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop first column since these appear to be entry numbers\n",
    "merged_df = merged_df.drop(merged_df.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that the Unnamed column has been dropped\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Noticed how 'condition' has some missing values, but other columns are fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values that are in 'condition'\n",
    "\n",
    "merged_df = merged_df.dropna(subset=['condition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduces the dataset to 213,869 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at unique drugs under 'Birth Control' condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['drugName'][merged_df['condition'] =='Birth Control'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df['drugName'][merged_df['condition'] =='Birth Control'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the the top 7 birth controls since we want to focus on these\n",
    "bc_drugs = ['Etonogestrel', 'Ethinyl estradiol / norethindrone', 'Nexplanon', 'Levonorgestrel', 'Ethinyl estradiol / levonorgestrel',\n",
    "           'Ethinyl estradiol / norgestimate', 'Implanon']\n",
    "\n",
    "bc_data = merged_df[merged_df['drugName'].isin(bc_drugs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bc_data['drugName'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wanted to only include in our dataset the top 7 drugs with condition = birth control, however when we filtered for these drugs we see additional conditions were selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bc_data['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's try to get rid of these conditions that took in the 'Useful' rating as 'condition'\n",
    "bc_data = bc_data[~bc_data['condition'].str.contains('comment')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bc_data['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data['drugName'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column called 'sentiment' that will have the target variables\n",
    "bc_data['sentiment'] = ['Positive' if x > 7.0 else 'Negative' for x in bc_data['rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the median rating based on condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "median_values = bc_data.groupby('condition')['rating'].median()\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize = (25, 10))\n",
    "ax.bar(median_values.index, median_values.values)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('Condition')\n",
    "ax.set_ylabel('Median Values')\n",
    "ax.set_title('Median Values by Group')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bc_data.loc[bc_data['condition'] == 'Emergency Contraception'].rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create columns that count emphasis and capital letters in text, as this could express sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a 'punc_emphasis' column that scores how many exclamation points and question marks are in the text\n",
    "\n",
    "bc_data['punc_emphasis'] = bc_data['review'].apply(lambda x: sum([1 for char in x if char in ['!', '?']]))\n",
    "\n",
    "### Creating a 'capt_emphasis' column that scores how many capitalized words are in the text\n",
    "\n",
    "bc_data['capt_emphasis'] = bc_data['review'].apply(lambda x: sum([1 for word in x.split() if word.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data['sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Against Condition\n",
    "- Within the conditions we have selected, how do the reviews look pertaining to each condition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try plotting sentiment against groups\n",
    "\n",
    "fig = plt.figure(figsize = (35, 10))\n",
    "\n",
    "hue_order = ['Positive', 'Negative']\n",
    "sns.countplot(x='condition', hue='sentiment', data=bc_data, hue_order=hue_order, palette='tab10', order = bc_data['condition'].value_counts().index)\n",
    "\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Conditions by Review Sentiment');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try plotting sentiment against groups\n",
    "\n",
    "fig = plt.figure(figsize = (25, 10))\n",
    "\n",
    "\n",
    "sns.countplot(x='drugName', hue='sentiment', data=bc_data, order = bc_data['drugName'].value_counts().index)\n",
    "\n",
    "# plt.xticks(rotation=45)\n",
    "plt.xlabel('Drug Names')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Birth Control Drugs by Review Sentiment');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we do anything with 'UsefulCount'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bc_data['usefulCount'].value_counts(bins=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data['usefulCount'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bc_data['usefulCount'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not sure if this is too useful of a feature, maybe we could filter the reviews that were found useful above a certain threshold to take in user input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After research on birth controls, wanted to read what reviews are saying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data[bc_data['drugName'] == 'Levonorgestrel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning the years these reviews were written into two groups to see if there's a difference over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data['date'] = pd.to_datetime(bc_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data['date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data['date'].value_counts(bins=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column called 'date_column' that will have grouped time ranges of 2008-2012 and 2013-2017\n",
    "bc_data['date_column'] = ['2013-2017' if x.year > 2013 else '2008-2012' for x in bc_data['date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we see the difference in reviews of these drugs over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try plotting sentiment of Levonorgestrel against the two assigned time periods\n",
    "\n",
    "fig = plt.figure(figsize = (25, 10))\n",
    "\n",
    "sns.countplot(x='date_column', hue='sentiment', data=bc_data[bc_data['drugName'] == 'Levonorgestrel'], hue_order=hue_order, palette='tab10')\n",
    "\n",
    "# plt.xticks(rotation=45)\n",
    "plt.xlabel('Time Periods')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Levonorgestrel Sentiment by Two Time Periods');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try plotting sentiment of Etonogestrel against the two assigned time periods\n",
    "\n",
    "fig = plt.figure(figsize = (25, 10))\n",
    "\n",
    "sns.countplot(x='date_column', hue='sentiment', data=bc_data[bc_data['drugName'] == 'Etonogestrel'], hue_order=hue_order, palette='tab10')\n",
    "\n",
    "# plt.xticks(rotation=45)\n",
    "plt.xlabel('Time Periods')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Etonogestrel Sentiment by Two Time Periods');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try plotting sentiment of Ethinyl estradiol / norethindrone against the two assigned time periods\n",
    "\n",
    "fig = plt.figure(figsize = (25, 10))\n",
    "\n",
    "sns.countplot(x='date_column', hue='sentiment', data=bc_data[bc_data['drugName'] == 'Ethinyl estradiol / norethindrone'], hue_order=hue_order, palette='tab10')\n",
    "\n",
    "# plt.xticks(rotation=45)\n",
    "plt.xlabel('Time Periods')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Ethinyl estradiol / norethindrone Sentiment by Two Time Periods');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Text Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that will lowercase the text\n",
    "\n",
    "def lower_case(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Create function to remove the html apostrophes in the text\n",
    "\n",
    "def apostrophe(text):\n",
    "    text = text.replace('&#039;', '\\'')\n",
    "    return text\n",
    "\n",
    "# Want to expand the contractions so we can see if these words have importance\n",
    "\n",
    "def fixcontractions(text):\n",
    "    text = contractions.fix(text)\n",
    "    return text\n",
    "\n",
    "# Create a function that uses a regex tokenizer to remove punctuation but ignores contraction apostrophes\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+\\'?\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "# Remove stopwords from the reviews\n",
    "\n",
    "def remove_stopwords(text, stop_words_list = set(stopwords.words('english'))):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if word not in stop_words_list]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "# Create a function that lemmatizes words\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = lower_case(text)\n",
    "    text = apostrophe(text)\n",
    "    text = fixcontractions(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually testing the contractions.fix function\n",
    "contractions.fix(\"I've aren't Tim's got a lovely bunch of coconuts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original review text location 6\n",
    "bc_data['review'][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing one of the reviews to see what it is doing to the text, as above\n",
    "clean_text(bc_data['review'][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting the top 10 most common words in the 'text' column in an sns bar chart after applying the clean_text function\n",
    "\n",
    "text = ' '.join(bc_data['review'])\n",
    "text = clean_text(text)\n",
    "text = text.split()\n",
    "\n",
    "freq = pd.Series(text).value_counts()[:10]\n",
    "freq = freq.to_frame()\n",
    "freq = freq.reset_index()\n",
    "freq.columns = ['word', 'count']\n",
    "freq = freq.sort_values(by='count', ascending=False)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "sns.barplot(x='count', y='word', data=freq, palette='tab10')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Word')\n",
    "plt.title('Top 10 Most Common Words in Reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Simple Model - Count Vectorizer / Logistic Regression / No Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X1 = bc_data['review']\n",
    "y1 = bc_data['sentiment']\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X1, y1, test_size=0.2, random_state=1337)\n",
    "\n",
    "# For Train Set, apply clean_text function\n",
    "\n",
    "X_train_1 = X_train_1.apply(clean_text)\n",
    "\n",
    "### Train - Tokenize the training data with a simple split of words, and then flattening to prepare for vectorization\n",
    "\n",
    "X_train_1 = X_train_1.apply(lambda x: x.split())\n",
    "X_train_1 = X_train_1.map(' '.join)\n",
    "\n",
    "### Train - Vectorize the training data using CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_1 = cv.fit_transform(X_train_1)\n",
    "\n",
    "### Train - Fit training data to Logistic Regression Model\n",
    "\n",
    "logit = LogisticRegression()\n",
    "logit.fit(X_train_1, y_train_1)\n",
    "\n",
    "### VALIDATION - Perform a cross validation on the logistic regression model\n",
    "\n",
    "scores = cross_val_score(logit, X_train_1, y_train_1, cv=5)\n",
    "print('Cross Validation Scores: ', scores)\n",
    "print('Mean Cross Validation Score: ', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Test Set Preprocessing\n",
    "\n",
    "X_test_1_logit = X_test_1.apply(clean_text)\n",
    "X_test_1_logit = X_test_1.apply(lambda x: x.split())\n",
    "X_test_1_logit = X_test_1.map(' '.join)\n",
    "X_test_1_logit = cv.transform(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logit_pred = logit.predict(X_test_1_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Logistic Regression Accuracy: ', accuracy_score(y_test_1, logit_pred))\n",
    "print('Logistic Regression F1 Score: ', f1_score(y_test_1, logit_pred, average='weighted'))\n",
    "print('Logistic Regression Precision Score: ', precision_score(y_test_1, logit_pred, average='weighted'))\n",
    "print('Logistic Regression Recall Score: ', recall_score(y_test_1, logit_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get scores for the sentiments individually to see\n",
    "\n",
    "# Quick look at decision matrix for our first model:\n",
    "\n",
    "### Logistic Regression Confusion Matrix\n",
    "\n",
    "cm = confusion_matrix(y_test_1, logit_pred)\n",
    "cm_df = pd.DataFrame(cm, index=['Negative', 'Positive'], columns=['Negative', 'Positive'])\n",
    "\n",
    "fig_cm1 = plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_df, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Logistic Regression Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math check to see if these add up to the train split. It does!\n",
    "1748 + 1975 + 362 + 391"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model - TFID Vectorizer / Logistic Regression / No FeaturesÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Want to test if TFIDF Vectorizer makes a difference compared to Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_tfidf = bc_data['review']\n",
    "y1_tfidf = bc_data['sentiment']\n",
    "\n",
    "X_train_1_tfidf, X_test_1_tfidf, y_train_1_tfidf, y_test_1_tfidf = train_test_split(X1, y1, test_size=0.2, random_state=1337)\n",
    "\n",
    "# For Train Set, apply clean_text function\n",
    "\n",
    "X_train_1_tfidf = X_train_1_tfidf.apply(clean_text)\n",
    "\n",
    "### Train - Tokenize the training data with a simple split of words, and then flattening to prepare for vectorization\n",
    "\n",
    "X_train_1_tfidf = X_train_1_tfidf.apply(lambda x: x.split())\n",
    "X_train_1_tfidf = X_train_1_tfidf.map(' '.join)\n",
    "\n",
    "### Train - Vectorize the training data using CountVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_1_tfidf = tfidf.fit_transform(X_train_1_tfidf)\n",
    "\n",
    "### Train - Fit training data to Logistic Regression Model\n",
    "\n",
    "logit_tfidf = LogisticRegression()\n",
    "logit_tfidf.fit(X_train_1_tfidf, y_train_1_tfidf)\n",
    "\n",
    "### VALIDATION - Perform a cross validation on the decision tree classifier\n",
    "\n",
    "scores = cross_val_score(logit_tfidf, X_train_1_tfidf, y_train_1_tfidf, cv=5)\n",
    "print('Cross Validation Scores: ', scores)\n",
    "print('Mean Cross Validation Score: ', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Test (TFIDF) Set Preprocessing\n",
    "\n",
    "X_test_1_tfidf = X_test_1_tfidf.apply(clean_text)\n",
    "X_test_1_tfidf = X_test_1_tfidf.apply(lambda x: x.split())\n",
    "X_test_1_tfidf = X_test_1_tfidf.map(' '.join)\n",
    "X_test_1_tfidf = cv.transform(X_test_1_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pred_tfidf = logit_tfidf.predict(X_test_1_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic Regression Confusion Matrix w/ TFIDF\n",
    "\n",
    "cm = confusion_matrix(y_test_1_tfidf, logit_pred_tfidf)\n",
    "cm_df = pd.DataFrame(cm, index=['Negative', 'Positive'], columns=['Negative', 'Positive'])\n",
    "\n",
    "fig_cm1 = plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_df, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Logistic Regression Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression (TFIDF) Accuracy: ', accuracy_score(y_test_1_tfidf, logit_pred_tfidf))\n",
    "print('Logistic Regression (TFIDF) F1 Score: ', f1_score(y_test_1_tfidf, logit_pred_tfidf, average='weighted'))\n",
    "print('Logistic Regression (TFIDF) Precision Score: ', precision_score(y_test_1_tfidf, logit_pred_tfidf, average='weighted'))\n",
    "print('Logistic Regression (TFIDF) Recall Score: ', recall_score(y_test_1_tfidf, logit_pred_tfidf, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see here that our Logistic regression model using TFIDF has a lower accuracy than the model with Count Vector. This could possibly be contributed to the fact of class imbalance, and the words that the model is selecting for is the result of the imbalanced classes. Will look at sampling differently in the next model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for best Logistic Regression parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [1, 10, 100, 1000, 10000],'penalty': ['none', 'l1', 'l2', 'elasticnet']}\n",
    "grid = GridSearchCV(logit, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_train_1, y_train_1)\n",
    "print('Best Parameters: ', grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks that the default parameters are the best parameters for this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Model: Count Vectorizer / Multinomial Bayes / No Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using the same X_train and y_train from our first and second models\n",
    "\n",
    "X1_nb = bc_data['review']\n",
    "y1_nb = bc_data['sentiment']\n",
    "\n",
    "X_train_1_nb, X_test_1_nb, y_train_1_nb, y_test_1_nb = train_test_split(X1, y1, test_size=0.2, random_state=1337)\n",
    "\n",
    "# For Train Set, apply clean_text function\n",
    "\n",
    "X_train_1_nb = X_train_1_nb.apply(clean_text)\n",
    "\n",
    "### Train - Tokenize the training data with a simple split of words, and then flattening to prepare for vectorization\n",
    "\n",
    "X_train_1_nb = X_train_1_nb.apply(lambda x: x.split())\n",
    "X_train_1_nb = X_train_1_nb.map(' '.join)\n",
    "\n",
    "### Train - Vectorize the training data using CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_1_nb_cv = cv.fit_transform(X_train_1_nb)\n",
    "\n",
    "# Train - fitting the training data to a Naive Bayes Classifier\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_1, y_train_1)\n",
    "\n",
    "# Validation - Performing cross validation on the Naive Bayes Classifier\n",
    "\n",
    "scores = cross_val_score(nb, X_train_1, y_train_1, cv=5)\n",
    "print('Cross Validation Scores: ', scores)\n",
    "print('Mean Cross Validation Score: ', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for best conditions with NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0.1, 0.5, 1, 2, 5, 20, 50]}\n",
    "grid = GridSearchCV(nb, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_train_1, y_train_1)\n",
    "print('Best Parameters: ', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Test Set Preprocessing\n",
    "\n",
    "X_test_1_nb = X_test_1_nb.apply(clean_text)\n",
    "X_test_1_nb = X_test_1_nb.apply(lambda x: x.split())\n",
    "X_test_1_nb = X_test_1_nb.map(' '.join)\n",
    "X_test_1_nb = cv.transform(X_test_1_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pred_cv = nb.predict(X_test_1_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Naive Bayes Confusion Matrix w/ CountVectorizer\n",
    "\n",
    "cm = confusion_matrix(y_test_1_nb, nb_pred_cv)\n",
    "cm_df = pd.DataFrame(cm, index=['Negative', 'Positive'], columns=['Negative', 'Positive'])\n",
    "\n",
    "fig_cm1 = plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_df, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Naive Bayes Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Naive Bayes Accuracy: ', accuracy_score(y_test_1_nb, nb_pred_cv))\n",
    "print('Naive Bayes F1 Score: ', f1_score(y_test_1_nb, nb_pred_cv, average='weighted'))\n",
    "print('Naive Bayes Precision Score: ', precision_score(y_test_1_nb, nb_pred_cv, average='weighted'))\n",
    "print('Naive Bayes Recall Score: ', recall_score(y_test_1_nb, nb_pred_cv, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4th Model: Count Vectorizer / Logistic Regression / Review, Drug Name, condition, punc emphasis, capt emphasis vs. Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = bc_data[['review', 'drugName', 'condition', 'punc_emphasis', 'capt_emphasis']]\n",
    "y2 = bc_data['sentiment']\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X2, y2, test_size=0.2, random_state=1337)\n",
    "\n",
    "# For Train Set, apply clean_text function\n",
    "\n",
    "X_train_2['review'] = X_train_2['review'].apply(clean_text)\n",
    "\n",
    "### Train - Tokenize the training data with a simple split of words, and then flattening to prepare for vectorization\n",
    "\n",
    "X_train_2['review'] = X_train_2['review'].apply(lambda x: x.split())\n",
    "X_train_2['review'] = X_train_2['review'].map(' '.join)\n",
    "\n",
    "### Train - Vectorize the training data using CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_2 = cv.fit_transform(X_train_2['review'])\n",
    "\n",
    "### Train - Fit training data to Logistic Regression Model\n",
    "\n",
    "logit = LogisticRegression()\n",
    "logit.fit(X_train_2, y_train_2)\n",
    "\n",
    "### VALIDATION - Perform a cross validation on the logistic regression model\n",
    "\n",
    "scores = cross_val_score(logit, X_train_2, y_train_2, cv=5)\n",
    "print('Cross Validation Scores: ', scores)\n",
    "print('Mean Cross Validation Score: ', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5th Model: Want to see training on Levonogestrel on its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lev = bc_data[bc_data['drugName'] == 'Levonorgestrel']\n",
    "data_lev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lev['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lev.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = data_lev['review']\n",
    "y3 = data_lev['sentiment']\n",
    "\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X3, y3, test_size=0.2, random_state=1337)\n",
    "\n",
    "# For Train Set, apply clean_text function\n",
    "\n",
    "X_train_3 = X_train_3.apply(clean_text)\n",
    "\n",
    "### Train - Tokenize the training data with a simple split of words, and then flattening to prepare for vectorization\n",
    "\n",
    "X_train_3 = X_train_3.apply(lambda x: x.split())\n",
    "X_train_3 = X_train_3.map(' '.join)\n",
    "\n",
    "### Train - Vectorize the training data using CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_3 = cv.fit_transform(X_train_3)\n",
    "\n",
    "### Train - Fit training data to Logistic Regression Model\n",
    "\n",
    "logit_lev = LogisticRegression()\n",
    "logit_lev.fit(X_train_3, y_train_3)\n",
    "\n",
    "### VALIDATION - Perform a cross validation on the logistic regression model\n",
    "\n",
    "scores = cross_val_score(logit_lev, X_train_3, y_train_3, cv=5)\n",
    "print('Cross Validation Scores: ', scores)\n",
    "print('Mean Cross Validation Score: ', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Test Set Preprocessing\n",
    "\n",
    "X_test_3_logit = X_test_3.apply(clean_text)\n",
    "X_test_3_logit = X_test_3.apply(lambda x: x.split())\n",
    "X_test_3_logit = X_test_3.map(' '.join)\n",
    "X_test_3_logit = cv.transform(X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pred_3 = logit_lev.predict(X_test_3_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression Accuracy: ', accuracy_score(y_test_3, logit_pred_3))\n",
    "print('Logistic Regression F1 Score: ', f1_score(y_test_3, logit_pred_3, average='weighted'))\n",
    "print('Logistic Regression Precision Score: ', precision_score(y_test_3, logit_pred_3, average='weighted'))\n",
    "print('Logistic Regression Recall Score: ', recall_score(y_test_3, logit_pred_3, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get scores for the sentiments individually to see\n",
    "\n",
    "# Quick look at decision matrix for our fifth model:\n",
    "\n",
    "### Logistic Regression Confusion Matrix\n",
    "\n",
    "cm = confusion_matrix(y_test_3, logit_pred_3)\n",
    "cm_df = pd.DataFrame(cm, index=['Negative', 'Positive'], columns=['Negative', 'Positive'])\n",
    "\n",
    "fig_cm3 = plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_df, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Logistic Regression Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
